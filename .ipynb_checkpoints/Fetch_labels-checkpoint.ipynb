{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486cf39a-3650-4a3c-926d-c598d05ecfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal, get_args\n",
    "from openpyxl import Workbook, load_workbook\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# new SDK:\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "# progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ─── CONFIG ───────────────────────────────────────────────────────────────────\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GEMINI_API_KEY:\n",
    "    raise RuntimeError(\"Missing environment variable: GOOGLE_API_KEY\")\n",
    "\n",
    "client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "\n",
    "# ─── TYPES & PROMPT ───────────────────────────────────────────────────────────\n",
    "\n",
    "SectorType = Literal[\n",
    "    \"Government/Public Sector\", \"Finance and Insurance\", \"Technology\",\n",
    "    \"Telecommunications\", \"Real Estate\", \"Healthcare\", \"Retail\",\n",
    "    \"Manufacturing\", \"Entertainment\", \"Education\", \"Energy\",\n",
    "    \"Automotive\", \"Hospitality\", \"Transportation and Logistics\",\n",
    "    \"Food and Beverage\", \"Nonprofit/NGO\", \"Agriculture\", \"Other\"\n",
    "]\n",
    "JudgmentOutcomeType = Literal[\"Plaintiff\", \"Defendant\", \"Undecided\"]\n",
    "\n",
    "sector_list = \", \".join(get_args(SectorType))\n",
    "system_prompt = (\n",
    "    \"You are a legal and business analyst.\\n\"\n",
    "    \"Analyze legal case texts and provide answers in JSON format with the keys \\\"sector\\\" and \\\"judgment_outcome\\\".\\n\\n\"\n",
    "    f\"1. Identify which USA tertiary sector the case belongs to from the following list: {sector_list}.\\n\"\n",
    "    \"   If none apply, respond with \\\"Other\\\".\\n\\n\"\n",
    "    \"2. Determine the judgment outcome: Plaintiff, Defendant, or Undecided.\\n\\n\"\n",
    "    \"Respond only with a JSON object. Example:\\n\"\n",
    "    \"{\\\"sector\\\": \\\"Technology\\\", \\\"judgment_outcome\\\": \\\"Plaintiff\\\"}\"\n",
    ")\n",
    "\n",
    "# ─── CLEANING ─────────────────────────────────────────────────────────────────\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    soup = BeautifulSoup(text, \"lxml\")\n",
    "    return soup.get_text(separator=\" \", strip=True)\n",
    "\n",
    "# ─── MODEL OUTPUT PARSING ─────────────────────────────────────────────────────\n",
    "\n",
    "class LegalAnalysisResult(BaseModel):\n",
    "    sector: SectorType = Field(..., description=\"The USA tertiary sector the case belongs to\")\n",
    "    judgment_outcome: JudgmentOutcomeType = Field(..., description=\"Plaintiff, Defendant, or Undecided\")\n",
    "\n",
    "def extract_sector_and_outcome(case_text: str) -> LegalAnalysisResult | None:\n",
    "    try:\n",
    "        resp = client.models.generate_content(\n",
    "            model=\"gemini-1.5-flash-latest\",\n",
    "            contents=[system_prompt, case_text],\n",
    "            config=types.GenerateContentConfig(\n",
    "                temperature=0.0,\n",
    "                top_k=1,\n",
    "                top_p=0.0,\n",
    "                max_output_tokens=512,\n",
    "                response_mime_type=\"application/json\"\n",
    "            )\n",
    "        )\n",
    "        return LegalAnalysisResult.parse_raw(resp.text)\n",
    "    except Exception as e:\n",
    "        if \"RESOURCE_EXHAUSTED\" in str(e):\n",
    "            print(\"Rate limit hit. Sleeping for 30 seconds before retry...\")\n",
    "            time.sleep(30)\n",
    "            return extract_sector_and_outcome(case_text)\n",
    "        print(f\"Gemini error processing case: {e}\")\n",
    "        return None\n",
    "\n",
    "# ─── EXCEL UTIL ───────────────────────────────────────────────────────────────\n",
    "\n",
    "def write_result_to_excel(file_path: str, row: list):\n",
    "    if not os.path.exists(file_path):\n",
    "        wb = Workbook()\n",
    "        ws = wb.active\n",
    "        ws.append([\"id\", \"sector\", \"judgment_outcome\"])\n",
    "        wb.save(file_path)\n",
    "\n",
    "    wb = load_workbook(file_path)\n",
    "    ws = wb.active\n",
    "    ws.append(row)\n",
    "    wb.save(file_path)\n",
    "\n",
    "# ─── MAIN ─────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def main():\n",
    "    input_file = \"/content/remaining.json\"\n",
    "    output_file = \"/content/analysis(remaining).xlsx\"\n",
    "\n",
    "     # Handle JSONDecodeError:\n",
    "    try:\n",
    "        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            cases = json.load(f)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "        # Attempt to fix the JSON:\n",
    "        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = f.read()\n",
    "            # Simple fix: replace unescaped newlines within strings:\n",
    "            data = data.replace(\"\\n\", \"\\\\n\")\n",
    "            try:\n",
    "                cases = json.loads(data)  # Try to load the fixed data\n",
    "            except json.JSONDecodeError as e2:\n",
    "                print(f\"Could not fix JSON: {e2}\")\n",
    "                return\n",
    "\n",
    "    # Wrap with tqdm, show total, update every record\n",
    "    for idx, case in enumerate(tqdm(cases, desc=\"Processing cases\", unit=\"case\"), start=1):\n",
    "        cid = case.get(\"id\")\n",
    "        text_raw = case.get(\"plain_text\", \"\")\n",
    "        text = clean_text(text_raw)\n",
    "        if not text:\n",
    "            tqdm.write(f\"Skipping {cid}: empty text\")\n",
    "            continue\n",
    "\n",
    "        analysis = extract_sector_and_outcome(text)\n",
    "        if analysis:\n",
    "            tqdm.write(f\"{cid} → Sector: {analysis.sector}, Outcome: {analysis.judgment_outcome}\")\n",
    "            write_result_to_excel(output_file, [cid, analysis.sector, analysis.judgment_outcome])\n",
    "        else:\n",
    "            tqdm.write(f\"{cid} → No result\")\n",
    "\n",
    "        # Print a special notice every 500 records\n",
    "        if idx % 500 == 0:\n",
    "            tqdm.write(f\"✅ {idx} records processed so far\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbf41d7-e7e6-43ac-b700-62de32aa623a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from openpyxl import Workbook, load_workbook\n",
    "\n",
    "# Gemini client imports\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "# ─── CONFIG ───────────────────────────────────────────────────────────────────\n",
    "GEMINI_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GEMINI_API_KEY:\n",
    "    raise RuntimeError(\"Missing environment variable: GOOGLE_API_KEY\")\n",
    "\n",
    "client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "\n",
    "# ─── PROMPT SETUP ─────────────────────────────────────────────────────────────\n",
    "system_prompt = (\n",
    "    \"You are a legal expert in USA.\\n\"\n",
    "    \"Each input is a semicolon‑separated string of act names.\\n\"\n",
    "    \"if the input is null or nan or empty, set coun to 0 and move to next row, do not count nans\"\n",
    "    \"if the input is non empty then Split on `;`, extract the unique act names, count them, and respond only in JSON \"\n",
    "    \"with these two keys:\\n\"\n",
    "    \"  1. \\\"unique_acts\\\" (an array of strings)\\n\"\n",
    "    \"  2. \\\"act_count\\\" (integer)\\n\"\n",
    "    \"Do not include any extra text or formatting.\"\n",
    ")\n",
    "\n",
    "def write_row_to_excel(output_path: str, row_id: str, row_json: dict):\n",
    "    \"\"\"\n",
    "    Append a single row to the Excel file, keyed by row_id.\n",
    "    Creates the file with header if it doesn't exist.\n",
    "    \"\"\"\n",
    "    cols = [\"id\", \"unique_acts\", \"act_count\"]\n",
    "    if not os.path.exists(output_path):\n",
    "        wb = Workbook()\n",
    "        ws = wb.active\n",
    "        ws.append(cols)\n",
    "        wb.save(output_path)\n",
    "\n",
    "    wb = load_workbook(output_path)\n",
    "    ws = wb.active\n",
    "    # Check if 'act_count' and 'unique_acts' are present in row_json\n",
    "    act_count = row_json.get(\"act_count\", 0)  # Default to 0 if not found\n",
    "    unique_acts = row_json.get(\"unique_acts\", [])  # Default to empty list if not found\n",
    "    ws.append([\n",
    "        row_id,\n",
    "        \", \".join(unique_acts),\n",
    "        act_count\n",
    "    ])\n",
    "    wb.save(output_path)\n",
    "\n",
    "def process_acts(acts_str: str) -> dict | None:\n",
    "    \"\"\"\n",
    "    Call Gemini 1.5 Flash to extract unique acts and count them.\n",
    "    Returns the parsed JSON dict, or None on failure.\n",
    "    \"\"\"\n",
    "    user_prompt = (\n",
    "        f\"Act names: {acts_str}\\n\"\n",
    "        \"(Values are separated by ';')\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        resp = client.models.generate_content(\n",
    "            model=\"gemini-1.5-flash-latest\",\n",
    "            contents=[system_prompt, user_prompt],\n",
    "            config=types.GenerateContentConfig(\n",
    "                temperature=0.0,\n",
    "                top_k=1,\n",
    "                top_p=0.0,\n",
    "                max_output_tokens=256,\n",
    "                response_mime_type=\"application/json\"\n",
    "            )\n",
    "        )\n",
    "        # Attempt to parse the response as JSON\n",
    "        # If parsing fails, return an empty dictionary with default values\n",
    "        try:\n",
    "            result = json.loads(resp.text)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"[Warning] Could not parse JSON response for acts_str: {acts_str}\")\n",
    "            result = {\"unique_acts\": [], \"act_count\": 0}\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] processing acts: {e}\")\n",
    "        if \"RESOURCE_EXHAUSTED\" in str(e):\n",
    "            time.sleep(30)\n",
    "            return process_acts(acts_str)\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    output_path = \"/content/output_acts_additional.xlsx\" # where results will be written\n",
    "\n",
    "    for _, r in acts_df.iterrows():\n",
    "        row_id   = r.get(\"id\", \"\")\n",
    "        acts_str = r.get(\"ACT_names\", \"\")\n",
    "        if not row_id or not acts_str:\n",
    "            print(f\"Skipping row id={row_id!r}: missing data\")\n",
    "            continue\n",
    "\n",
    "        result = process_acts(acts_str)\n",
    "        if result:\n",
    "            write_row_to_excel(output_path, row_id, result)\n",
    "            print(f\"Wrote id={row_id} (count={result.get('act_count', 0)})\")\n",
    "        else:\n",
    "            print(f\"Failed to process id={row_id}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee20f716-a084-41d1-8a89-aa0662c0716b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "from openpyxl import Workbook, load_workbook\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "# ─── CONFIG ───────────────────────────────────────────────────────────────────\n",
    "GEMINI_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GEMINI_API_KEY:\n",
    "    raise RuntimeError(\"Missing environment variable: GOOGLE_API_KEY\")\n",
    "client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "\n",
    "# ─── PROMPT ───────────────────────────────────────────────────────────────────\n",
    "system_prompt = (\n",
    "    \"You are a legal expert in USA.\\n\"\n",
    "    \"Each input is a semicolon‑separated string of act names.If there is no semicolon, check the text if it has any act in it\\n\"\n",
    "    \"If the input is null, nan, or empty, respond with:\\n\"\n",
    "    \"  { \\\"unique_acts\\\": [], \\\"act_count\\\": 0 }\\n\"\n",
    "    \"Otherwise, split on `;`, dedupe, count, and respond ONLY with JSON:\\n\"\n",
    "    \"{\\n\"\n",
    "    \"  \\\"unique_acts\\\": [ ... ],\\n\"\n",
    "    \"  \\\"act_count\\\": integer\\n\"\n",
    "    \"}\\n\"\n",
    "    \"No extra text.\"\n",
    ")\n",
    "\n",
    "def write_row_to_excel(output_path: str, row_id: str, data: dict):\n",
    "    cols = [\"id\", \"unique_acts\", \"act_count\"]\n",
    "    if not os.path.exists(output_path):\n",
    "        wb = Workbook()\n",
    "        ws = wb.active\n",
    "        ws.append(cols)\n",
    "        wb.save(output_path)\n",
    "\n",
    "    wb = load_workbook(output_path)\n",
    "    ws = wb.active\n",
    "    ws.append([\n",
    "        row_id,\n",
    "        \", \".join(data.get(\"unique_acts\", [])),\n",
    "        data.get(\"act_count\", 0)\n",
    "    ])\n",
    "    wb.save(output_path)\n",
    "\n",
    "def process_acts(acts_str: str) -> dict:\n",
    "    user_prompt = f\"Act names: {acts_str}\\n(Values are separated by ';')\"\n",
    "    resp = client.models.generate_content(\n",
    "        model=\"gemini-1.5-flash-latest\",\n",
    "        contents=[system_prompt, user_prompt],\n",
    "        config=types.GenerateContentConfig(\n",
    "            temperature=0.0,\n",
    "            top_k=1,\n",
    "            top_p=0.0,\n",
    "            max_output_tokens=1024,\n",
    "            response_mime_type=\"application/json\"\n",
    "        )\n",
    "    )\n",
    "    raw = resp.text.strip()\n",
    "    # grab first {...} block\n",
    "    m = re.search(r'\\{.*\\}', raw, flags=re.DOTALL)\n",
    "    js = m.group() if m else raw\n",
    "    try:\n",
    "        return json.loads(js)\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"[Warning] Could not parse JSON:\\n{raw}\\nDefaulting to zero.\")\n",
    "        return {\"unique_acts\": [], \"act_count\": 0}\n",
    "\n",
    "def main():\n",
    "    output_path = \"output_acts.xlsx\"\n",
    "\n",
    "    # iterate over the already‐loaded problem_rows DataFrame\n",
    "    for _, row in problem_rows.iterrows():\n",
    "        row_id   = str(row.get(\"id\", \"\")).strip()\n",
    "        acts_str = str(row.get(\"ACT_names\", \"\")).strip()\n",
    "\n",
    "        if not row_id:\n",
    "            print(\"Skipping row with missing id\")\n",
    "            continue\n",
    "\n",
    "        # Always show the original text\n",
    "        print(f\"\\nRow id={row_id}\")\n",
    "        print(f\"Original ACT_names: {acts_str!r}\")\n",
    "\n",
    "        if not acts_str:\n",
    "            data = {\"unique_acts\": [], \"act_count\": 0}\n",
    "            print(\"→ Empty input, count=0\")\n",
    "        else:\n",
    "            data = process_acts(acts_str)\n",
    "            # Show the extracted unique acts\n",
    "            print(f\"→ Extracted unique_acts ({len(data.get('unique_acts', []))}):\")\n",
    "            for act in data.get(\"unique_acts\", []):\n",
    "                print(f\"   - {act}\")\n",
    "\n",
    "        write_row_to_excel(output_path, row_id, data)\n",
    "        print(f\"Wrote to Excel: id={row_id}, act_count={data.get('act_count', 0)}\")\n",
    "\n",
    "        time.sleep(0.1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7413658a-9673-4112-b80c-42bdd07ce2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "case_law_metadata = pd.read_excel(r\"/content/drive/MyDrive/FHA/Thesis_data/caselaw_meta.xlsx\")\n",
    "citations_analysis_final = pd.read_excel(r\"/content/drive/MyDrive/FHA/Thesis_data/citations_analysis_final.xlsx\")\n",
    "\n",
    "# Normalize column names if needed\n",
    "case_law_metadata = case_law_metadata.rename(columns={\"opinion_id\": \"id\"})\n",
    "\n",
    "# Perform an outer join on ['id', 'opinion_id']\n",
    "merged = pd.merge(\n",
    "    citations_analysis_final,\n",
    "    case_law_metadata,\n",
    "    on=\"id\",\n",
    "    how=\"outer\",\n",
    "    indicator=True  # optional: adds a column \"_merge\" showing the source of each row\n",
    ")\n",
    "\n",
    "merged = merged.drop(columns=[\"_merge\"])\n",
    "\n",
    "label_analysis_final = pd.read_excel(r\"/content/drive/MyDrive/FHA/Thesis_data/label_analysis_final.xlsx\")\n",
    "\n",
    "final_df = pd.merge(\n",
    "    merged,\n",
    "    label_analysis_final,\n",
    "    on=\"id\",\n",
    "    how=\"outer\",\n",
    "    indicator=True\n",
    ")\n",
    "\n",
    "final_df = final_df.drop(columns=[\"_merge\"])\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# prepare lists to collect the counts\n",
    "statute_counts  = []\n",
    "case_law_counts = []\n",
    "rules_counts    = []\n",
    "\n",
    "for _, row in final_df[['STATUTE_codes', 'CASE_LAW_names', 'RULES_codes']].iterrows():\n",
    "    # STATUTE_codes\n",
    "    sc = row['STATUTE_codes']\n",
    "    if pd.isna(sc):\n",
    "        statute_counts.append(0)\n",
    "    else:\n",
    "        s_list = sc.split(';')\n",
    "        # use pandas' unique instead of set\n",
    "        statute_counts.append(len(pd.unique(s_list)))\n",
    "\n",
    "    # CASE_LAW_names\n",
    "    cl = row['CASE_LAW_names']\n",
    "    if pd.isna(cl):\n",
    "        case_law_counts.append(0)\n",
    "    else:\n",
    "        cl_list = cl.split(';')\n",
    "        case_law_counts.append(len(pd.unique(cl_list)))\n",
    "\n",
    "    # RULES_codes\n",
    "    rc = row['RULES_codes']\n",
    "    if pd.isna(rc):\n",
    "        rules_counts.append(0)\n",
    "    else:\n",
    "        r_list = rc.split(';')\n",
    "        rules_counts.append(len(pd.unique(r_list)))\n",
    "\n",
    "# attach back to final_df\n",
    "final_df['unique_STATUTE_codes']   = statute_counts\n",
    "final_df['unique_CASE_LAW_names']  = case_law_counts\n",
    "final_df['unique_RULES_codes']     = rules_counts\n",
    "\n",
    "\n",
    "try:\n",
    "  df_add = pd.read_excel(\"/content/output1_acts_updated.xlsx\")\n",
    "except FileNotFoundError:\n",
    "  print(\"Error: File '/content/output1_acts_updated.xlsx' not found.\")\n",
    "  # Handle the error appropriately, e.g., exit or use a default DataFrame\n",
    "  exit()\n",
    "\n",
    "# Merge the DataFrames on 'id'\n",
    "final_df = pd.merge(final_df, df_add, on='id', how='left')\n",
    "\n",
    "final_df = final_df.drop(columns=['ACT_names'])\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Save the final_df DataFrame to your Google Drive\n",
    "final_df.to_excel('/content/drive/MyDrive/FHA/Thesis_data/final_df.xlsx', index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
