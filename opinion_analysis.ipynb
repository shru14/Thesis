{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555c93d0-544f-46de-92b6-e06c7fa9169c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import gc\n",
    "import nltk\n",
    "import jsonlines\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from eyecite import get_citations\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "def load_data(file_path):\n",
    "    data = []\n",
    "    try:\n",
    "        with jsonlines.open(file_path) as reader:\n",
    "            for obj in reader:\n",
    "                data.append(obj)\n",
    "    except jsonlines.InvalidLineError:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            if not isinstance(data, list) or not all(isinstance(item, dict) for item in data):\n",
    "                raise ValueError(\"Input file is not in a valid JSON or JSON Lines format.\")\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    if 'plain_text' not in df.columns:\n",
    "        raise ValueError(\"Column 'plain_text' not found in the data\")\n",
    "    df[\"plain_text\"] = df[\"plain_text\"].astype(str).str.strip()\n",
    "    return df\n",
    "\n",
    "def fast_clean_text(text):\n",
    "    soup = BeautifulSoup(text, \"lxml\")\n",
    "    return soup.get_text(separator=\" \", strip=True)\n",
    "\n",
    "def extract_citations(text):\n",
    "    citations = get_citations(text)\n",
    "    grouped = {}\n",
    "    for cit in citations:\n",
    "        cit_type = type(cit).__name__\n",
    "        if cit_type == \"FullLawCitation\":\n",
    "            key = \"STATUTE\"\n",
    "        elif cit_type == \"FullCaseCitation\":\n",
    "            key = \"CASE_LAW\"\n",
    "        elif cit_type in (\"IdCitation\", \"SupraCitation\"):\n",
    "            continue\n",
    "        else:\n",
    "            key = \"OTHER\"\n",
    "        grouped.setdefault(key, []).append(cit)\n",
    "\n",
    "    rules_pattern = r\"\\bRule(?:s)?\\s+(\\d+)(?:\\((\\w+)\\))?(?:\\((\\w+)\\))?(?:\\s+of\\s+(?:the\\s+)?Federal\\s+Rules(?:\\s+of\\s+(Civil|Criminal|Appellate|Evidence))?)?\"\n",
    "    rule_matches = re.findall(rules_pattern, text, re.IGNORECASE)\n",
    "    rules_extractions = [\"Rule \" + \" \".join(part for part in match if part) for match in rule_matches]\n",
    "    if rules_extractions:\n",
    "        grouped.setdefault(\"RULES\", []).extend(rules_extractions)\n",
    "\n",
    "    return grouped\n",
    "\n",
    "def get_statute_codes(citation_results):\n",
    "    codes = []\n",
    "    for cit in citation_results:\n",
    "        try:\n",
    "            if hasattr(cit, 'groups'):\n",
    "                title = cit.groups.get('title', '').strip()\n",
    "                reporter = cit.groups.get('reporter', '').strip()\n",
    "                section = cit.groups.get('section', '').strip()\n",
    "                if title and reporter and section:\n",
    "                    code = f\"{title} {reporter} ยง {section}\"\n",
    "                    if \"FullLawCitation\" not in code:\n",
    "                        codes.append(code)\n",
    "                    continue\n",
    "                else:\n",
    "                    code = str(cit).split(\"(\")[0].strip()\n",
    "                    if \"FullLawCitation\" not in code:\n",
    "                        codes.append(code)\n",
    "            else:\n",
    "                code = str(cit).split(\"(\")[0].strip()\n",
    "                if \"FullLawCitation\" not in code:\n",
    "                    codes.append(code)\n",
    "        except Exception:\n",
    "            code = str(cit).split(\"(\")[0].strip()\n",
    "            if \"FullLawCitation\" not in code:\n",
    "                codes.append(code)\n",
    "    return codes\n",
    "\n",
    "def get_act_names_from_text(text):\n",
    "    act_pattern = r\"([A-Z][\\w\\s]+ Act)\"\n",
    "    matches = re.findall(act_pattern, text)\n",
    "    return list(set(match.strip() for match in matches if match.strip()))\n",
    "\n",
    "def get_case_law_names_from_text(text):\n",
    "    pattern = r'\\b([A-Z][A-Za-z0-9,\\-\\.]+\\s+v\\.\\s+[A-Z][A-Za-z0-9,\\-\\.]+)\\b'\n",
    "    matches = re.findall(pattern, text)\n",
    "    return list(set(match.strip() for match in matches))\n",
    "\n",
    "def get_rule_citations(citation_results):\n",
    "    return [str(cit).strip() for cit in citation_results]\n",
    "\n",
    "def calculate_citation_density(text, citations):\n",
    "    sentences = sent_tokenize(text)\n",
    "    total_citations = sum(len(c) for c in citations.values())\n",
    "    return total_citations / len(sentences) if sentences else 0.0\n",
    "\n",
    "def calculate_lexical_complexity(text):\n",
    "    words = [w.lower() for w in text.split() if w.isalpha()]\n",
    "    return len(set(words)) / len(words) if words else 0.0\n",
    "\n",
    "def calculate_hedging_frequency(text):\n",
    "    hedgers = {\n",
    "        \"might\", \"could\", \"may\", \"seem\", \"appear\", \"suggest\", \"possibly\", \"probably\", \"likely\",\n",
    "        \"perhaps\", \"almost\", \"somewhat\", \"relatively\", \"tend to\", \"in some cases\", \"to some extent\",\n",
    "        \"it is possible\", \"it appears\", \"it seems\", \"suggests that\", \"indicates that\", \"would\", \"should\"\n",
    "    }\n",
    "    words = text.lower().split()\n",
    "    return sum(1 for w in words if w in hedgers) / len(words) if words else 0.0\n",
    "\n",
    "def count_ultra_vires(text):\n",
    "    return text.lower().count(\"ultra vires\")\n",
    "\n",
    "def count_chevron_deference(text):\n",
    "    keywords = [\"chevron deference\", \"chevron doctrine\", \"chevron step\"]\n",
    "    text_lower = text.lower()\n",
    "    return sum(text_lower.count(keyword) for keyword in keywords)\n",
    "\n",
    "def process_row(row):\n",
    "    opinion_text = row[\"plain_text\"]\n",
    "    if not opinion_text or opinion_text.isspace():\n",
    "        return None\n",
    "    try:\n",
    "        cleaned_text = fast_clean_text(opinion_text)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "    citations = extract_citations(cleaned_text)\n",
    "    raw_statute = \"; \".join([str(cit).split(\"(\")[0].strip() for cit in citations.get(\"STATUTE\", [])])\n",
    "    raw_case_law = \"; \".join([str(cit).split(\"(\")[0].strip() for cit in citations.get(\"CASE_LAW\", [])])\n",
    "    raw_rules = \"; \".join([str(cit).strip() for cit in citations.get(\"RULES\", [])])\n",
    "\n",
    "    statute_codes = get_statute_codes(citations.get(\"STATUTE\", []))\n",
    "    act_names = get_act_names_from_text(cleaned_text)\n",
    "    case_law_names = get_case_law_names_from_text(cleaned_text)\n",
    "    rule_codes = get_rule_citations(citations.get(\"RULES\", []))\n",
    "\n",
    "    citation_counts = {\n",
    "        \"STATUTE\": len(statute_codes),\n",
    "        \"ACT\": len(act_names),\n",
    "        \"CASE_LAW\": len(case_law_names),\n",
    "        \"RULES\": len(rule_codes)\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"id\": row[\"id\"],\n",
    "        \"unique_STATUTE_count\": citation_counts[\"STATUTE\"],\n",
    "        \"STATUTE_codes\": \"; \".join(statute_codes),\n",
    "        \"unique_ACT_count\": citation_counts[\"ACT\"],\n",
    "        \"ACT_names\": \"; \".join(act_names),\n",
    "        \"unique_CASE_LAW_count\": citation_counts[\"CASE_LAW\"],\n",
    "        \"CASE_LAW_names\": \"; \".join(case_law_names),\n",
    "        \"unique_RULES_count\": citation_counts[\"RULES\"],\n",
    "        \"RULES_codes\": \"; \".join(rule_codes),\n",
    "        \"raw_STATUTE\": raw_statute,\n",
    "        \"raw_CASE_LAW\": raw_case_law,\n",
    "        \"raw_RULES\": raw_rules,\n",
    "        \"citation_density\": calculate_citation_density(cleaned_text, citations),\n",
    "        \"citation_diversity\": sum(1 for count in citation_counts.values() if count > 0),\n",
    "        \"lexical_complexity\": calculate_lexical_complexity(cleaned_text),\n",
    "        \"hedging_frequency\": calculate_hedging_frequency(cleaned_text),\n",
    "        \"ultra_vires_count\": count_ultra_vires(cleaned_text),\n",
    "        \"chevron_deference_count\": count_chevron_deference(cleaned_text),\n",
    "    }\n",
    "\n",
    "def process_all_rows(df):\n",
    "    with Pool(processes=cpu_count() - 1) as pool:\n",
    "        results = list(tqdm(pool.imap(process_row, df.to_dict('records'), chunksize=10), total=len(df)))\n",
    "    return pd.DataFrame([r for r in results if r is not None])\n",
    "\n",
    "def main(file_path=\"/content/drive/MyDrive/MUN/chunk_1.json\", output_csv_path=None):\n",
    "    df = load_data(file_path)\n",
    "    df_results = process_all_rows(df)\n",
    "\n",
    "    if output_csv_path is None:\n",
    "        base_name = file_path.rsplit(\".\", 1)[0]\n",
    "        output_csv_path = f\"{base_name}_output.csv\"\n",
    "\n",
    "    df_results.to_csv(output_csv_path, index=False)\n",
    "    print(\"Processed results (first 5 rows):\")\n",
    "    print(df_results.head())\n",
    "    print(f\"Saved processed results to {output_csv_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
